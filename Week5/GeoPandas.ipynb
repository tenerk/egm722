{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9483aa48-7f8e-4377-981e-8ecdb61b6041",
   "metadata": {},
   "source": [
    "# additional work with geopandas\n",
    "\n",
    "## overview\n",
    "In this exercise, we'll get some additional practice using `geopandas` and `pandas` to explore spatial (and non-spatial) datasets.\n",
    "\n",
    "## objectives\n",
    "- use `pandas` string operations to apply **str** methods to columns of tables\n",
    "- expand on spatial joins, using representative points instead of centroids to join polygons\n",
    "- get additional practice with `.groupby` operations\n",
    "- see how we can join/merge tables using attributes\n",
    "- compare ways of iterating over **DataFrame** objects\n",
    "- use some of the `pandas` built-in plotting tools\n",
    "\n",
    "\n",
    "## data provided\n",
    "\n",
    "In the data\\_files folder, you should have the following files:\n",
    "-  schools_data.csv\n",
    "-  transport.csv\n",
    "\n",
    "We will also make use of the county outlines and 2011 wards boundaries used in previous weeks. \n",
    "\n",
    "## getting started\n",
    "\n",
    "To get started, run the cell below to import both `pandas` and `geopandas`, and load the two spatial datasets we will use at the start. When we load these datasets, we're making sure to re-project them to the same CRS ([EPSG:2157](https://epsg.io/2157), Irish Transverse Mercator) - that way, when we want to plot (or join) the datasets together, we know that they're in the same coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e99bf2-3d43-49ce-bccb-1884eebb2441",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counties = gpd.read_file('../Week2/data_files/Counties.shp').to_crs('epsg:2157')\n",
    "wards = gpd.read_file('../Week3/data_files/NI_Wards.shp').to_crs('epsg:2157')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4c0e3a-a95c-4202-b2d2-cff5ce6d7bf2",
   "metadata": {},
   "source": [
    "Let's take a look at the `CountyNames` column of our county attribute table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65bf844-a8fb-4a13-a716-ecd84e9bac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties['CountyName'] # show the county names column from the counties dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe677cb-e51c-4d74-a157-37f571d163a8",
   "metadata": {},
   "source": [
    "As we output our data for further analysis, including formatting figures, we might not want the county names to be in [shouting case](https://en.wikipedia.org/wiki/All_caps) - that is, we might want to update these to not be in all caps.\n",
    "\n",
    "We have previously seen how we can use *vectorized* operations on a single column (or multiple columns) of numerical data, but what about **str** objects? For an individual string, we can use a method such as `.title()` ([documentation](https://docs.python.org/3/library/stdtypes.html#str.title)), which converts the string to \"title\" case (first letter of each word is capitalized, other letters are lowercase). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff53bbc-ad56-43ec-8964-ce857f4eca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "'TYRONE'.title() # convert the string TYRONE to Tyrone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54fff18-eafe-4041-8345-23dc5f9e7e73",
   "metadata": {},
   "source": [
    "As we have discussed previously, we could iterate over the items in the **Series** - for example, using a *list comprehension*:\n",
    "\n",
    "```python\n",
    "not_shouting = [name.title() for name in counties['CountyName']]\n",
    "```\n",
    "\n",
    "Another, potentially easier way to do this, is by using `pandas` **str** methods ([documentation](https://pandas.pydata.org/docs/user_guide/text.html#string-methods)). Using the `.str` attribute of (certain types of) **Series** objects, we can use **str** methods such as `.title()`, which will operate on all of the items in the **Series**. For example, to convert each string value in a **Series** to title case, we can use `.str.title()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10072eb1-e124-4542-8ae2-8983d7c4ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties['CountyName'].str.title() # convert each string in the series to title case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203dc582-fb78-4adf-9fe1-7899158df74b",
   "metadata": {},
   "source": [
    "To update the `CountyName` column, we can assign the output of `.str.title()` to this column of the **DataFrame**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf92aaf-37e4-40e6-a5e7-63e65a7ff052",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties['CountyName'] = counties['CountyName'].str.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e949960d-4fc9-4fd2-a269-09cc291a54cc",
   "metadata": {},
   "source": [
    "Note that the `.str` attribute is only available if the **Series** is of type **object** (or **string**) - it won't work on numeric values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedae92b-262f-4f11-ae50-eac6a5e0afbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counties['Area_SqKM'].str.lower() # this won't work, because it's not a string!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95ff010-582e-40ff-a179-42163d1ed014",
   "metadata": {},
   "source": [
    "## spatial joins, revisited\n",
    "\n",
    "Now that we've further introduced vectorized operations, let's take a moment to remind ourselves what data we're working with. The two files that we have loaded so far, `counties` and `wards`, represent the boundaries of the six counties of Northern Ireland, and the 2011 Census wards and their population, respectively.\n",
    "\n",
    "To visualize these, we can use the `.plot()` method for a **GeoDataFrame** ([documentation](https://geopandas.org/en/latest/docs/reference/api/geopandas.GeoDataFrame.plot.html)), which allows us to make a chloropleth map based on spatial data. To show the outlines of the counties, we'll first use `.boundary` ([documentation](https://geopandas.org/en/latest/docs/reference/api/geopandas.GeoSeries.boundary.html)), which returns a **GeoSeries** of **LineString** objects representing the exterior boundary of the polygons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4fdf0b-f682-4bd0-8b2c-bc3347dd2628",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties.boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f6f04f-0d89-4efe-9ec9-8b311a0ca7d6",
   "metadata": {},
   "source": [
    "Putting this all together, we can make a plot that shows the outline of each ward, colored by the population (stretched to saturate between between 1000 and 8000). And, we'll plot the county outlines as a thin red line on the same axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a3684b-d348-4510-bc84-4c4041a7d607",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1) # create a figure and axis object to plot the data into\n",
    "\n",
    "wards.plot(column='Population', ax=ax, vmin=1000, vmax=8000, cmap='viridis')\n",
    "counties.boundary.plot(ax=ax, color='r', linewidth=0.4)\n",
    "\n",
    "ax.set_yticks([]) # turn off the yticks for visibility\n",
    "ax.set_xticks([]) # turn off the xticks for visibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d18fe6b-ea66-4f0a-a504-89da349f61e7",
   "metadata": {},
   "source": [
    "As we saw in a previous exercise, we can perform a spatial join using `.sjoin()` ([documentation](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.sjoin.html)) to join the electoral wards to the county (or counties) that they intersect. Unfortunately, as we also saw, the wards dataset do not fit neatly inside of the county boundaries, in part because of differences in digitizing.\n",
    "\n",
    "To double check this, let's join the wards to the counties, then compare (a) the number of items in the original dataset to the number of items in the joined datasets; and (b) the total population from the original dataset compared to the total population from the joined dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d2555e-6391-4fca-b122-a5c8db5a3030",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_polygon = counties.sjoin(wards) # join the two datasets using a basic spatial join\n",
    "\n",
    "print(f\"Number of electoral wards: {len(wards)}\")\n",
    "print(f\"Number of joined wards: {len(joined_polygon)}\")\n",
    "print('') # prints a blank line\n",
    "print(f\"Total population from wards: {wards['Population'].sum()}\")\n",
    "print(f\"Total population from joined: {joined_polygon['Population'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ef19cb-f3c4-44f5-ba91-8e8e45c81b09",
   "metadata": {},
   "source": [
    "From ths, it's clear that we're double-counting lots of wards: from the 582 original wards, we now have 702 in the joined dataset. This (not surprisingly) gives us a total population of 2.21 million, an increase of 21% from the original 1.81 million counted in the 2011 census.\n",
    "\n",
    "When we are joining two different polygon datasets, it is sometimes preferable to convert one of the datasets to a set of points. This is especially useful in cases where datasets may have been digitized without [snapping](https://www.geographyrealm.com/what-is-snapping-in-gis/) the vertices together, to avoid having gaps or overlaps between features.\n",
    "\n",
    "Let's try the (obvious) example first, where we use the *centroid*, or centerpoint, of each of the polygons. **GeoDataFrame** and **GeoSeries** objects have a `.centroid` attribute ([documentation](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoSeries.centroid.html)), which gives us a **GeoSeries** of **Point** objects corresponding to the center of each geometry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bad882-8e22-4c47-8708-c0bb4b5986d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wards.centroid # show the centroids of the wards geodataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437322fc-56e3-4646-b027-6041ed48b1ff",
   "metadata": {},
   "source": [
    "To visualize this **GeoSeries**, we'll plot the outlines of the wards dataset (again using `.boundary`), along with the centroids of each ward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99c60b9-4ae7-4a7d-a822-1dcb1e108e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = wards.boundary.plot(color='k') # plot the outlines of the counties\n",
    "wards.centroid.plot(ax=ax) # plot the centroids of each ward\n",
    "\n",
    "ax.set_yticks([]) # turn off the yticks for visibility\n",
    "ax.set_xticks([]) # turn off the xticks for visibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ab483f-4399-413b-8ae1-a12d7f4b4a31",
   "metadata": {},
   "source": [
    "And here, we see one of the potential pitfalls of using the centroid (noted, in fact, at the very top of the documentation page linked above):\n",
    "\n",
    "> Note that centroid does not have to be on or within original geometry.\n",
    "\n",
    "You can see this most clearly for Bonamargy and Rathlin in the northernmost part of the map above. Because this ward is split between two features ([Rathlin Island and part of the town of Ballycastle](https://www.openstreetmap.org/#map=12/55.2558/-6.2262)), the centerpoint ends up being somewhere between them in Rathlin Sound.\n",
    "\n",
    "In fact, there are a number of wards where the centroid is not actually within the original geometry - we can view this by using `.loc` along with `.contains()` ([documentation](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoSeries.contains.html)):\n",
    "\n",
    "```python\n",
    "wards['geometry'].contains(wards.centroid)\n",
    "```\n",
    "\n",
    "This gives us a boolean (True/False) **Series**, with a value of `True` where the original feature contains its centroid, and a value of `False` otherwise. To view the opposite, we can use the `~` (\"bitwise negation\") operator, which will invert the selection to show us only the rows where the centroid is not contained in the original feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df2933-96cc-499e-a8e0-b5085e4edf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "wards.loc[~wards['geometry'].contains(wards.centroid)] # show the wards whose centroid is not contained within the boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ff4c6-2822-43bc-a930-15f4bb967532",
   "metadata": {},
   "source": [
    "We can see that in fact there are 5 different wards where the centroid is not contained in the original feature.\n",
    "\n",
    "Furthermore, some centroids may not even fall within the county outlines - something that we can check using `.within()` ([documentation](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoSeries.within.html)). Similar to `.contains()`, `.within()` returns a boolean **Series** with a value of `True` where the original feature is *within* (i.e., fully contained inside of) some other geometry or **GeoSeries**.\n",
    "\n",
    "To check whether the centroids fall within *any* of the county boundaries, we can use `.union_all()` ([documentation](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoSeries.union_all.html)), which returns the union of all of the geometries within a **GeoSeries**.\n",
    "\n",
    "The following cell will show the wards whose centroid is not contained within any of the county boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ba99ea-dffa-4e67-ab79-2999289f415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wards.loc[~wards.centroid.within(counties.union_all())] # show the wards whose centroid is not contained within the county boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d4128-11ee-4cbd-bb96-2986398fad2d",
   "metadata": {},
   "source": [
    "As we might have suspected, the centroid of Bonamargy and Rathlin, which is located somewhere in Rathlin Sound, is not contained within a county outline - meaning that if we were to join using the centroids, we would be working with an incomplete dataset.\n",
    "\n",
    "Fortunately, we do have another way to do this, using `.representative_point()` ([documenation](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoSeries.representative_point.html)). A \"representative point\" is a point that is guaranteed to be within the original geometry, typically (but not always!) near the middle of the original feature.\n",
    "\n",
    "First, let's plot the representative points for each ward, alongside the ward outlines and centroids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa05e54-d8f5-4625-bdcb-c63ea39d5084",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = wards.boundary.plot(color='k') # plot the outlines of the counties\n",
    "wards.representative_point().plot(ax=ax) # plot the representative point of each ward\n",
    "wards.centroid.plot(marker='.', ax=ax) # plot the centroid as a small dot\n",
    "\n",
    "ax.set_yticks([]) # turn off the yticks for visibility\n",
    "ax.set_xticks([]) # turn off the xticks for visibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75e533-c1fa-4e0c-80e2-bf5964f0ef11",
   "metadata": {},
   "source": [
    "For most of the wards, we can see that the representative point and the centroid are in a similar enough location. Now, let's use `.copy()` ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.copy.html)) to create a copy of the original wards **GeoDataFrame**, then replace the `geometry` of that **GeoDataFrame** with the set of representative points. \n",
    "\n",
    "Because later on, we will also want to make use of the area of each ward, we will also add this as a column, using the `.area` attribute of the **GeoSeries** ([documentation](https://geopandas.org/en/latest/docs/reference/api/geopandas.GeoSeries.area.html)). Note that the `.area` attribute is calculated using the CRS of the **GeoSeries** - you'll want to make sure that the dataset is in a *projected* CRS before using this!\n",
    "\n",
    "Finally, we will perform the spatial join and check the number of features and total population calculated from the joined datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ee28cc-766e-4837-b2b3-a4d2df735fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wards_point = wards.copy()\n",
    "wards_point['geometry'] = wards.representative_point()\n",
    "wards_point['area'] = wards['geometry'].area\n",
    "\n",
    "joined_point = counties.sjoin(wards_point) # join the two datasets using a basic spatial join\n",
    "\n",
    "print(f\"Number of electoral wards: {len(wards)}\")\n",
    "print(f\"Number of joined wards: {len(joined_point)}\")\n",
    "print('')\n",
    "print(f\"Total population from wards: {wards['Population'].sum()}\")\n",
    "print(f\"Total population from joined: {joined_point['Population'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e2d26f-0a36-41ff-aaca-465ebd4377d5",
   "metadata": {},
   "source": [
    "So now we have joined the wards together with the counties, and the population (and number of features) in the joined dataset matches the original values. With this, we can move on to the next step(s) of our analysis, and look at how we can perform joins on non-spatial attributes.\n",
    "\n",
    "## non-spatial joins/merges\n",
    "\n",
    "`pandas` (and, by extension, `geopandas`) offers two main methods for combining tables based on (non-spatial) attributes:\n",
    "\n",
    "- `pd.merge()` (and `DataFrame.merge()`) ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html))\n",
    "- `DataFrame.join()` ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html))\n",
    "\n",
    "There are (mostly) minor differences between them; `.merge()` is slightly more flexible and is the underlying function used for `.join()`, so we will show examples using this.\n",
    "\n",
    "### indexing\n",
    "\n",
    "First, though, let's see how we can use the `index` to add information to a table. In most of the examples that we have seen so far, the `index` of the **DataFrame** has been an integer, usually corresponding to the row number. When we add a **Series** to a **DataFrame**, the values of the **Series** are mapped to the values of the `index` of the **DataFrame**.\n",
    "\n",
    "To illustrate this more concretely, let's look at an example. We'll first create an empty **DataFrame** with an `index` that ranges from 0 to 3 (remember that `range()` doesn't include the endpoint!).\n",
    "\n",
    "Then, we'll create two sets of values:\n",
    "\n",
    "- `ordered`, a **list** of the letters a through d;\n",
    "- `disordered`, a **Series** that uses the same values as `ordered`, but specifies a different order for the `index` values.\n",
    "\n",
    "Before running the cell below, be sure to think about what the output should look like. How do you think the two columns of the **DataFrame** will look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6162067c-0ca8-42ba-8f0d-7f74d86484e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index=range(0, 4))\n",
    "ordered = ['a', 'b', 'c', 'd']\n",
    "disordered = pd.Series(data=ordered, index=[3, 0, 2, 1])\n",
    "\n",
    "df['ordered'] = ordered\n",
    "df['disordered'] = disordered\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c83a498-9ec2-4965-a333-43a8ca0c8c8d",
   "metadata": {},
   "source": [
    "As we can see from the output above, when we add something to a **DataFrame** without specifying an `index` (i.e., when we add a **list** of values), it defaults to using a numeric `index` that is the same as the index of the original **list**: starting from 0 and incrementing by 1. So, the `index` values of `df['ordered']` are 0, 1, 2, and 3, in that order.\n",
    "\n",
    "However, we can also specify the `index` values when we create the **Series**, as with `disordered` above. When we do this, and then add `disordered` to the **DataFrame**, we can see that the values are placed in the row of the **DataFrame** corresponding to their `index` - so, 'a' (with an `index` of 3) gets placed in the final row of the **DataFrame**, 'b' (with an `index` of 0) gets placed in the first row, and so on.\n",
    "\n",
    "Taking this one step further, if we have a dataset with a unique identifier for each row (for example, the `Ward Code`, which uniquely identifies each ward), we can use this as an `index`. Then, when we want to add new data to our table in the form of a **Series**, as long as that **Series** uses the same index values as our **DataFrame**, it will add the **Series** values to the **DataFrame** in the correct order.\n",
    "\n",
    "To show that this works, let's first sort `joined_point` by the ward name using `.sort_values()` ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ea670a-0e57-4293-b6f1-8738078771c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_point.sort_values('Ward', inplace=True)\n",
    "joined_point # show that the table is now sorted by ward name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9586e74-fd58-47eb-944a-2b35f34da44c",
   "metadata": {},
   "source": [
    "Next, we'll use `.set_index()` ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html)) to make `Ward Code` the `index` of both `joined_point` and `wards`. Then, we'll add the name of the county where each ward is located (`CountyName`) to the ward **GeoDataFrame**. We should see that, even though `joined_point` has been sorted, the `County` column in the ward **GeoDataFrame** keeps the original (non-sorted) order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e39d28-c8ea-4056-aaf5-f8eb017fca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_point.set_index('Ward Code', inplace=True)\n",
    "wards.set_index('Ward Code', inplace=True)\n",
    "\n",
    "wards['County'] = joined_point['CountyName']\n",
    "wards # show the wards dataset, with the new column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017ed8db-9753-4a69-b193-6e3e8e224419",
   "metadata": {},
   "source": [
    "If you look at the order of `joined_point` that we saw previously, you should be able to see that the order of `joined_point['CountyName']` is not the same as the order of `wards['County']`: the first three county names are Antrim, Armagh, and Antrim, whereas the first three county names in `wards['County']` are all Antrim.\n",
    "\n",
    "### types of joins/merges\n",
    "\n",
    "So far, we've seen how we can use `index` values to add information to a (**Geo**)**DataFrame**. But, we might not always have a clear one-to-one relationship between two tables - we might have a one-to-many relationship, where a single row in one table corresponds to multiple rows in another table. In those cases, we'll use something like `pd.merge()`, which allows us to merge rows of tables together using different index-like values.\n",
    "\n",
    "The example dataset that we'll work with here is a compilation of school and student numbers for each of our different electoral wards. Using the [school location dataset](https://www.opendatani.gov.uk/@department-of-education/locate-a-school) provided by OpenDataNI, I have summarized the number of schools (divided into primary schools, non-grammar secondary schools, and grammar schools) found in each electoral ward, along with the total number of students in those categories. I also used the [library locations](https://www.opendatani.gov.uk/@libraries-ni/library-locations-ni)  dataset to count the number of libraries found in each electoral ward.\n",
    "\n",
    "To get started, let's first read **data_files/schools_data.csv** as a `pandas` **DataFrame**, then view what this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240267d3-2635-4196-bdf3-c71dfba1bb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schools_data = pd.read_csv('data_files/schools_data.csv')\n",
    "schools_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e0c5a8-8633-4339-a6f1-dbaeb76010de",
   "metadata": {},
   "source": [
    "Let's start by looking at what happens when we use `pd.merge()`. At a minimum, we need to specify `left_df` and `right_df` - in this case, `wards_point` and `schools_data`. We also want to make sure that we're merging using `Ward Code`, so we pass that as the `on` parameter. \n",
    "\n",
    "Note that if we don't specify `on`, `pd.merge()` uses the intersection of the columns of the two **DataFrame**s in order to do the merge - unless you're absolutely sure that there is only one column that is shared between the two **DataFrame**s, and that there are common values in that column in each **DataFrame**, it's better to be explicit!\n",
    "\n",
    "Run the following cell to see what the output of the merge looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d6bfd-155a-4262-8dc5-dc799ac00d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(wards_point, schools_data, on='Ward Code')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a3fc67-9900-4875-ace4-842863c80d9f",
   "metadata": {},
   "source": [
    "Our resulting table only has 486 rows in it - we've lost almost 100 rows from our original wards table. \n",
    "\n",
    "To figure out why this is, let's look at the types of join that we have available. From the documentation linked above, the default value for the `how` parameter of `pd.merge()` is `'inner'`, meaning that by default, `pd.merge()` uses an \"inner\" join. What is an \"inner\" join? We can see an explanation from the documentation linked above:\n",
    "\n",
    "> - **inner**: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys.\n",
    "\n",
    "So an \"inner\" join uses the intersection of keys from both frames. In our example above, the result of our merge operation is only those 486 wards that have at least one school or library - wards without a school or library are not included in **schools_data.csv**, so we don't have them in our final, merged, table.\n",
    "\n",
    "To see how we can merge the two dataframes but still keep wards without schools or libraries, let's look at the list of all of the accepted values of `how` that we can use to tell `pd.merge()` how to merge the two **DataFrame**s:\n",
    "\n",
    "> - **left**: use only keys from left frame, similar to a SQL left outer join; preserve key order.\n",
    "> - **right**: use only keys from right frame, similar to a SQL right outer join; preserve key order.\n",
    "> - **outer**: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically.\n",
    "> - **inner**: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys.\n",
    "> - **cross**: creates the cartesian product from both frames, preserves the order of the left keys.\n",
    "\n",
    "One of these in particular stands out - the `'outer'` join, which uses the union of keys from both **DataFrame**s. We could also use the `'right'` join, though if there are keys in the left **DataFrame** that aren't in the right **DataFrame**, we end up losing information as well. That's not an issue in this case, since the right **DataFrame** contains all possible ward codes, but it's something to keep in mind for other datasets.\n",
    "\n",
    "Let's see what the output of `pd.merge()` looks like when we specify `how='outer'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e4c06-56db-4a07-8f22-a5e2f96a5fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(wards_point, schools_data, on='Ward Code', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150e500b-cab8-4b19-acd9-1af19b3fc1ad",
   "metadata": {},
   "source": [
    "Here, we can see one other potential issue: by default, when `pd.merge()` adds a row where values are missing in one of the **DataFrame**s, it inserts those values as `NaN` (\"not a number\"). Among other things, this can mean that calculations involving those columns end up with `NaN` values.\n",
    "\n",
    "In general, the way to handle `NaN` or missing values is potentially an entire module of its own, as it has different implications for the resulting calculations. `pandas` has a [good explainer](https://pandas.pydata.org/docs/user_guide/missing_data.html) for how missing values propagate through different calculations, and different ways to handle them. You should think carefully about whether and how to fill, ignore, or drop missing values on a case-by-case basis, based on why those values are missing.\n",
    "\n",
    "Here, because we know that those values are missing because there are no schools or libraries in those wards, we will use `.fillna()` ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html)) to give those cells a value of 0. Finally, we will cast the output of this as a **GeoDataFrame**, to help preserve the spatial dimension of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3426de9f-0854-4339-bead-229a56e6cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "wards_schools = gpd.GeoDataFrame(pd.merge(wards_point, schools_data, on='Ward Code', how='outer').fillna(0))\n",
    "wards_schools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40db5043-efa0-4e55-91ad-eb595c75fcbb",
   "metadata": {},
   "source": [
    "Before we join our school and ward dataset to the county dataset, let's first take a moment to add two additional columns, using `.sum()` ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sum.html)).\n",
    "\n",
    "The first column, `'schools'`, will be the total number of schools (of any type) in the ward. The second column, `'students'`, will be the total number of students (of any type) in the ward. To calculate these, we first have to select those columns that represent the three types of schools (or students), then calculate the sum using `.sum()`.\n",
    "\n",
    "Note, however, that the default behavior of `.sum()` is to calculate the sum across rows; here, we want to calculate the sum across columns, so that the end result is the number of schools (or students) in each ward. To do that, we need to pass `axis=1` to `.sum()`, as you can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24680612-6a3c-4234-82ed-e8893fd57507",
   "metadata": {},
   "outputs": [],
   "source": [
    "wards_schools['schools'] = wards_schools[['primary_schools', 'grammar_schools', 'secondary_schools']].sum(axis=1)\n",
    "wards_schools['students'] = wards_schools[['primary_students', 'grammar_students', 'secondary_students']].sum(axis=1)\n",
    "\n",
    "wards_schools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad5d341-5125-466f-952b-b9820a98a7d4",
   "metadata": {},
   "source": [
    "Hopefully, in looking at the examples above, you can see that this has worked - the value of `'schools'` in row 579 is 2, as this ward has one primary school and one secondary school; similarly, there are 2161 students, based on 622 primary students and 1539 secondary students.\n",
    "\n",
    "Finally, we are ready to perform a spatial join of our combined wards and schools datasets, with the county outlines. When we do this, we will make sure to only select the relevant columns from `counties` (`CountyName`, `Area_SqKM`, and `geometry`). We'll then set the `Ward Code` as the `index` for the **GeoDataFrame**, and remove the `index_right` column since we don't need to keep track of the original row number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136c43ee-a20a-4802-8a4f-1886265b2734",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_schools = counties[['CountyName', 'Area_SqKM', 'geometry']].sjoin(wards_schools)\n",
    "county_schools.set_index('Ward Code', inplace=True) # set the index to be the ward code\n",
    "county_schools.drop(columns=['index_right'], inplace=True) # drop the original index from our wards_schools dataset\n",
    "\n",
    "county_schools # show the joined dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f21c1ef-ecb9-4bad-88e6-3c9a0dbb3341",
   "metadata": {},
   "source": [
    "## summarizing and grouping datasets\n",
    "\n",
    "Now that we have finished preparing our dataset, let's work on starting to analyze what we have. First, we'll have a look at `.describe()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html)), which provides a summary of each of the (numeric) columns in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa5fbc-2873-44c7-a763-a83cf750c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_schools.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f7df8b-8364-46ca-90ec-af2043d4eb13",
   "metadata": {},
   "source": [
    "In the output above, we can see the count (**count**) minimum (**min**), 1st quartile (**25%**), median (**50%**), mean (**mean**), 3rd quartile (**75%**), maximum (**max**), and standard deviation (**std**) values of each numeric variable in the table.\n",
    "\n",
    "With this, we can quickly see where we might have errors in our data - for example, if we have non-physical or nonsense values in our variables. When first getting started with a dataset, it can be a good idea to check over the dataset using `.describe()`, if you are using it in an interactive environment (such as a jupyter notebook).\n",
    "\n",
    "Next, we'll see how we can use different tools to aggregate and summarize our data, starting with `.groupby()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html)), which allows us to aggregate the values in the table by grouping rows based on the values found in one or more columns.\n",
    "\n",
    "To start, we'll group the data by `CountyName`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0849bab1-5cca-4836-8770-bc185f2eb9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_groups = county_schools.groupby('CountyName') # create a grouped dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3283a0-7f6f-40e7-84d1-94feae7a5c77",
   "metadata": {},
   "source": [
    " The output of `.groupby()` is a **DataFrameGroupBy** object, which we can then use to do different calculations based on the groups created. These work in similar ways to a **DataFrame** - for example, we can select an individual column (like `Population`) and calculate the `.sum()` based on which county each ward is located within:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8e5c59-3350-48df-8cf0-e61c9b0ae893",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_groups['Population'].sum() # get a summary of the population for each county"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49eaf7e-13c8-420e-82aa-efcde7cb85e0",
   "metadata": {},
   "source": [
    "When we only use a single column from the table, the output is a **Series** with the `index` equal to whatever values make up the groups - in this case, the name of each county.\n",
    "\n",
    "This means that we can start to build a **DataFrame** that summarizes different columns from our original table, using the county names as an `index`. We'll start with the population, as calculated above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68f5834-c950-4fd8-8d9e-8ccd6e07e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame(index=counties['CountyName']) # create a new summary dataframe\n",
    "summary['population'] = county_groups['Population'].sum() # get the total population of each county\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3472704b-11c9-4168-a5b2-89d31c730ea3",
   "metadata": {},
   "source": [
    "Next, we can add the area of each county (in square km), and calculate the population density of that county by dividing the population by the area (note that these are *vectorized* operations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20dda74-a4b8-4512-b35a-0b6df59ab2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary['area'] = counties.set_index('CountyName')['geometry'].area / 1e6 # get the area of each county in square km\n",
    "summary['density'] = summary['population'] / summary['area'] # calculate population density as the population divided by the area\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6c2fba-4c7f-43f1-97b1-7e6ec7883fc2",
   "metadata": {},
   "source": [
    "Then, we can add additional calculations such as the total number of primary schools in each county, as well as the number of primary schools per 1000 residents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b58c5f5-6e31-4278-90b7-6b66e9e801ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary['primary_schools'] = county_groups['primary_schools'].sum()\n",
    "summary['primary_schools_per_capita'] = summary['primary_schools'] / (summary['population'] / 1000)\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665130ec-800f-4cdb-a11b-40156d27a171",
   "metadata": {},
   "source": [
    "... and so on. You should be able to adapt the code snippets shown above to start to work on some of the suggested practice exercises listed at the end of the notebook, to answer some different questions about what this dataset shows.\n",
    "\n",
    "## iterrows vs. itertuples\n",
    "\n",
    "One other thing that we'll look at is how we can *iterate* over the rows of a **DataFrame**. Previously, we have seen how in many cases, we can use *vectorized* operations to avoid needing to do this. That said, there are still some cases where we might need to, so let's have a look at two different ways to do this: `.iterrows()` ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iterrows.html)) and `.itertuples()` ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.itertuples.html)).\n",
    "\n",
    "The main differences are:\n",
    "- `.iterrows()` converts each row into a **Series**, and the **iterator** returns both the `index` and **Series** of each row.\n",
    "- `.itertuples()` converts each row into a **namedtuple** ([documentation](https://docs.python.org/3/library/collections.html#collections.namedtuple)), which is returned by the **iterator**\n",
    "\n",
    "`.itertuples()` tends to be a bit faster than `.iterrows()` (because converting the row into a **Series** is a bit slower than converting it into a **namedtuple**). Let's look at how `.iterrows()` works first, by iterating over the 5 wards with the most schools, and printing some information about them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f5440f-1b86-45cd-8b16-6cb966402d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The wards with the most schools are:')\n",
    "print('')\n",
    "\n",
    "top_schools = county_schools.sort_values('schools', ascending=False).head()\n",
    "\n",
    "for ind, row in top_schools.iterrows():\n",
    "    print(f\"{ind}, {row['Ward']}, County {row['CountyName']}: {int(row['schools'])} schools and {int(row['students'])} students.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524231d0-46d3-4f6c-a63d-16ad7338f909",
   "metadata": {},
   "source": [
    "Note the definition of our `for` loop:\n",
    "\n",
    "```python\n",
    "for ind, row in top_schools.iterrows():\n",
    "```\n",
    "\n",
    "Because the iterating variable of `.iterrows()` is (`index`, **Series**) pairs, we typically use two variables in the definition (here, `ind` and `row`). Inside of the `for` loop, this means that we can make use of both of these variables, and they will be updated each step of the loop.\n",
    "\n",
    "Notice also that the `index` values of the `row` **Series** are the names of each of the columns of the original **DataFrame** - we can access the individual values from each column of the row using the original column names.\n",
    "\n",
    "For `.itertuples()`, the iterating variable is a **namedtuple** of the values of the row. We can access the values of a **namedtuple** in two ways:\n",
    "\n",
    "- using the index value (0, 1, ...), exactly the same way as we would a **tuple**;\n",
    "- as an **attribute** of the **namedtuple**: for example, `row.column_name`.\n",
    "\n",
    "Here is the same loop as we saw previously, but this time using `.itertuples()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ccdef3-cb90-451b-85f3-d58271901873",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The wards with the most schools are:')\n",
    "print('')\n",
    "\n",
    "for ward in top_schools.itertuples():\n",
    "    print(f\"{ward.Index}, {ward.Ward}, County {ward.CountyName}: {int(ward.schools)} schools and {int(ward.students)} students.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c979d07a-1baa-4848-a2c1-0ec87651b7d5",
   "metadata": {},
   "source": [
    "Note that accessing the values of the **namedtuple** as an **attribute** only works if the original column names don't have spaces in them, so it's important to make sure that your column names don't have spaces!\n",
    "\n",
    "To illustrate what happens if there are spaces, we'll use `.rename()` ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html)) to add a space to the `CountyName` column label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ce0542-b633-48bb-acb8-7afcc406df96",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_schools.rename(columns={'CountyName': 'County Name'}, inplace=True)\n",
    "\n",
    "for ward in top_schools.itertuples(name='Ward'):\n",
    "    print(ward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77741b6-c777-4b26-a22c-37f2ffa758ba",
   "metadata": {},
   "source": [
    "Here, you can see that `'County Name'` has become `'_1'`, which isn't nearly as helpful as something like `'CountyName'`; this is why it's generally a good idea to avoid having spaces for the column names of your **DataFrame** (or **GeoDataFrame**). If you are working with datasets that do have spaces in the column names (or row index), you can use something like the following code to replace spaces with underscores (`'_'`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4878eb8-8ebc-46f5-a3d2-840ce995ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_names = top_schools.columns # get the column names of the dataframe\n",
    "new_names = [c.replace(' ', '_') for c in old_names] # replace any space characters with an underscore\n",
    "\n",
    "top_schools.rename(columns=dict(zip(old_names, new_names)), inplace=True) # use rename to rename the columns\n",
    "top_schools # show the updated dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571a78e5-2d79-4228-aa52-994871eef7fa",
   "metadata": {},
   "source": [
    "This uses a *list comprehension* to replace any space characters with an underscore in each column names (and if there aren't any, it returns the original column name). Then, like we have seen before, we use the output of `zip()` to create a **dict** that we can pass to the `columns` parameter of `.rename()` and update the column names accordingly.\n",
    "\n",
    "## plotting data\n",
    "\n",
    "The last thing that we will look at is how to plot some of the results from our **DataFrame**. In the previous exercise, we saw how we can use `matplotlib` directly to plot some of our data. `pandas` (and, as we have seen, `geopandas`) (**Geo**)**DataFrame** objects have a `.plot()` method which allows us to plot our data, without needing to use the `matplotlib` plotting routines directly.\n",
    "\n",
    "The generic `.plot()` has a number of different plot types that it will produce, using the `kind` parameter:\n",
    "\n",
    "- **line**: a line plot (also `.plot.line()`) ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.line.html))\n",
    "- **bar**: a vertical bar plot (also `.plot.bar()`) ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.bar.html))\n",
    "- **barh**: a horizontal bar plot (also `.plot.barh()`) ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.barh.html))\n",
    "- **hist**: a histogram (also `.plot.hist()` and `.hist()`) ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.hist.html))\n",
    "- **box**: a boxplot (also `.plot.box()` and `.boxplot()`) ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.box.html))\n",
    "- **kde**: a Kernel Density Estimation plot (also `.plot.kde()`) ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.kde.html))\n",
    "- **density**: same as **kde** (also `.plot.density()`) ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.density.html))\n",
    "- **area**: an area plot (also `.plot.area()`) ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.area.html))\n",
    "- **pie**: a pie plot (also `.plot.pie()`) ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.pie.html))\n",
    "- **scatter**: a scatter plot (also `.plot.scatter()`) ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.scatter.html))\n",
    "- **hexbin**: a hexbin plot (also `.plot.hexbin()`) ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.hexbin.html))\n",
    "\n",
    "Let's have a look at an example using `.hist()`, to show the distribution of the number of schools in each ward. We use the `column` parameter to tell `pandas` which column(s) from our **DataFrame** we want to show the distribution of, and we'll use `range()` to create the bins of our histogram, to range from 0 up to 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d078e86-de00-4524-8b41-6c27d3e286ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_schools.hist(column='schools', bins=range(0, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87208f0-6f84-4a58-bcd6-5af5860c6853",
   "metadata": {},
   "source": [
    "If we instead want to compare the histogram for some category (for example, by county), we can use the `by` parameter to tell `pandas` how to group the data before plotting. This will create a separate subplot for each value in the category (i.e., one for each county). \n",
    "\n",
    "Note that if we do this, we might also want to use `sharey=True`, so that each panel has the same y-axis so that we can more easily compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc8c980-8b9a-4597-a1de-353d1cc1e876",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_schools.hist(column='schools', by='CountyName', bins=range(0, 8), sharey=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc2184-e084-44f3-bdf3-3958728ba51b",
   "metadata": {},
   "source": [
    "Finally, let's use our `summary` **DataFrame** to compare the population of each county against the number of primary schools per 1000 residents, using `.plot.scatter()`. \n",
    "\n",
    "We'll then assign the output of `.plot.scatter()`, which is a **matplotlib.axes.Axes** object ([documentation](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.html)), so that we can use `.set_ylabel()` ([documentation](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set_ylabel.html)) and `.set_xlabel()` ([documentation](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set_xlabel.html)) to change the default axis labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb45d80-9145-4bc7-9d76-0586e8ac00f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = summary.plot.scatter(x='population', y='primary_schools_per_capita') # make a scatter plot of primary schools per 1000 residents vs population\n",
    "\n",
    "ax.set_ylabel('Primary Schools per 1000 Residents') # set the y-axis label\n",
    "ax.set_xlabel('Population') # set the x-axis label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1b83c4-9780-4b0b-9dbc-6aaf09cb46fc",
   "metadata": {},
   "source": [
    "So far, we've seen how to make basic plots using both `matplotlib` and the `pandas`/`geopandas` interface to `matplotlib` functionality. `matplotlib` is a very flexible (almost too flexible) package for making charts and figures, with loads of customizability that you can use to enhance your figures. \n",
    "\n",
    "Another package that you might want to have a look at is `seaborn` ([documentation](https://seaborn.pydata.org/)), which is built on top of `matplotlib` and provides a high-level interface for data visualization, using similar syntax to `ggplot2` for the **R** programming language. `seaborn` makes some of the more common customizations much easier than using `matplotlib` or the `pandas` interface, while still providing an easy interface for working with (**Geo**)**DataFrame** objects. \n",
    "\n",
    "## next steps\n",
    "\n",
    "That's all for this practical exercise. If you would like some additional practice, use the datasets that you have already loaded to try to answer the following questions:\n",
    "\n",
    "- what percentage of each county's population are students in primary/grammar/secondary school?\n",
    "- which county has the most schools (all types) per capita? is it different for each type of school?\n",
    "- make a (interactive) map that shows the number of schools in each ward - do you see any differences in the number of schools between urban and rural wards?\n",
    "- what is the total population who live in a ward with no schools?\n",
    "\n",
    "### even more practice\n",
    "\n",
    "For even more additional practice, try the following. In the **data_files** folder, there is an additional file, **transport.csv**, which contains information about the ways that people in each ward travel to work or school. From left to right, the columns tell the number of residents who:\n",
    "\n",
    "- **residents**: are in school full-time (primary or older), or in work full-time (ages 16-74)\n",
    "- **work_from_home**: work or study primarily from home\n",
    "- **train**: primarily take the train to/from work or study\n",
    "- **bus**: primarily take a bus/minibus/coach to/from work or study\n",
    "- **motorcycle**: primarily take a motorcycle, scooter, or moped to/from work or study\n",
    "- **driving**: primarily drive to/from work or study\n",
    "- **passenger**: primarily ride in a private car to/from work or study\n",
    "- **carpool**: primarily participate in a carpool to/from work or study\n",
    "- **taxi**: primarily take a taxi to/from work or study\n",
    "- **bicycle**: primarily take a bicycle to/from work or study\n",
    "- **walking**: primarily walk to/from work or study\n",
    "- **other**: primarily take some other form of transportation to/from work or study\n",
    "- **public**: primarily take public transportation (e.g., train or bus) to/from work or study\n",
    "\n",
    "Load this dataset, then merge it to your existing wards data. Then, try to answer the following questions:\n",
    "- which county has the highest percentage out of all residents who use a bicycle to get to/from work?\n",
    "- which county has the highest percentage out of all residents who use a bicycle to get to/from work?\n",
    "- does there appear to be a relationship between bicycle use and public transportation use?\n",
    "- for each ward, calculate the percentage of residents who study/work full-time who primarily walk to/from school/work. Then, compare the histograms of the percentage of residents who walk between wards with at least one primary school to the wards without a primary school. Does there appear to be a difference between these two distributions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
